
<!DOCTYPE html>
<html>
        <head>
                <title>Ionut-Teodor Sorodoc</title>
                <!-- link to main stylesheet -->
                <link rel="stylesheet" type="text/css" href="/main.css">
        </head>
        <body>
                <nav>
                <ul>

                        <li><a href="/">Home</a></li>
                        <li><a href="/CV/">CV</a></li>
                        <li><a href="/Publications/">Publications</a></li>
                        <li><a href="/Presentations/">Presentations</a></li>

                </ul>
                </nav>
                <div class="container">
                <div class="blurb">
                        <h2>Publications</h2>
			<h2>2025</h2>
			<ul>
			<li>Coman, A., <b> Sorodoc, I.</b>, Ribeiro, L. F., Byrne, B., Henderson, J., de Gispert, A. (2025).  (<i>to appear</i>).  RAGferee: Building Contextual Reward Models for Retrieval-Augmented Generation, Main conference EMNLP 2025. </li>
</li>		

			<li><b> Sorodoc, I.*</b>, Ribeiro, L. F., Blloshmi, R., Davis C., de Gispert, A. (2025).  GaRAGe: A Benchmark with Grounding Annotations for RAG Evaluation,  Findings of ACL 2025. <a href="https://arxiv.org/abs/2506.07671" target="_blank">[paper]</a> <a href="https://huggingface.co/datasets/AmazonScience/GaRAGe" target="_blank">[data&code]</a></li>
</li>
			</ul>
			<h2>2024</h2>
			<ul>
			<li>Christmann, P., Vakulenko, S., <b> Sorodoc, I.</b>, Byrne, B., de Gispert, A. (2024).
 (<i>to appear</i>). Challenges in including extra-linguistic context in pre-trained language models, Findings of EMNLP 2024. <a href="https://aclanthology.org/2024.findings-emnlp.835/" target="_blank">[paper]</a> </li>
			</ul>
			<h2>2022</h2>
			<ul>
			<li><b> Sorodoc, I.*</b>, Aina, L., Boleda, G. (2022).
 Challenges in including extra-linguistic context in pre-trained language models, Workshop on Insights from Negative Results in NLP 2022. <a href="https://aclanthology.org/2022.insights-1.18/" target="_blank">[paper]</a>  </li>
			</ul>                        
			<h2>2021</h2>
			<ul>
			<li><b> Sorodoc, I.*</b>, Boleda, G., Baroni, M. (2021). Controlled tasks for model analysis: Retrieving discrete information from sequences, BlackboxNLP 2021. <a href="https://aclanthology.org/2021.blackboxnlp-1.37/" target="_blank">[paper]</a> <a href="poster_bbnlp2021.pdf" target="_blank">[poster]</a>  </li>
			</ul>                        
			<h2>2020</h2>
			<ul>
			<li><b> Sorodoc, I.*</b>, Gulordava, K., Boleda, G. (2020).
 Probing for referential information in language models,  ACL 2020. <a href="https://aclanthology.org/2020.acl-main.384/" target="_blank">[paper]</a><a href="acl202_presentation.pdf" target="_blank">[slides]</a> </li>
			</ul>                        

			<h2>2019</h2>
			<ul>
			<li>Herrera, A., Ventura, C., Silberer, C., <b> Sorodoc, I.</b>, Boleda, G., Giro, X. (2019).  Recurrent Instance Segmentation using Sequences of Referring Expressions, ViGiL Workshop 2019. <a href="https://vigilworkshop.github.io/static/papers/30.pdf" target="_blank">[paper]</a> </li>
</li>		

			<li>Aina, L.*, Silberer, C.*, <b> Sorodoc, I.*</b>, Westera, M.*, Boleda, G. (2019).  What do entity-centric models learn? Insights from entity linking in multi-party dialogue,  NAACL 2019. <a href="https://www.aclweb.org/anthology/N19-1378.pdf" target="_blank">[paper]</a> <a href="https://github.com/amore-upf/analysis-entity-centric-nns" target="_blank">[data&code]</a></li>
</li>
			</ul>


			<h2>2018</h2>
			<ul>
			
			<li>Aina, L.*, Silberer, C.*, <b> Sorodoc, I.*</b>, Westera, M.*, Boleda, G. (2018).
 (<i>to appear</i>).  AMORE-UPF at SemEval-2018 Task 4: BiLSTM with entity library, <b>SemEval-2018</b>.  <a href="https://repositori.upf.edu/bitstream/handle/10230/35468/boleda_semeval2018_amoretask4.pdf?sequence=1&isAllowed=y" target="_blank">[paper]</a> <a href="https://github.com/amore-upf/semeval2018-task4" target="_blank">[data&code]</a></li>


			<li>Pezzelle, S.,<b> Sorodoc, I.</b>, Bernardi, R. (2018). Comparatives, Quantifiers, Proportions: A Multi-Task Model for the Learning of Quantities from Vision, <b>NAACL-HLT2018</b>. <a href="http://aclweb.org/anthology/N18-1039" target="_blank">[paper]</a> <a href="bib/naacl2018.bib" target="_blank">[bib]</a> <a href="https://github.com/sandropezzelle/multitask-quant" target="_blank">[data&code]</a> <a href="Pezzelle_NAACL2018.pdf" target="_blank">[slides]</a> <a href="poster_caos_def.pdf" target="_blank">[poster]</a> <a href="https://arxiv.org/pdf/1804.05018.pdf" target="_blank">[arxiv]</a></li>

			<li><b> Sorodoc, I.</b>, Pezzelle, S., Dimiccoli, M., Herbelot, A., Bernardi, R. (2018). Learning Quantification from Images: A Structured Neural Architecture. <b>JNLE2018</b>. <a href="https://www.cambridge.org/core/services/aop-cambridge-core/content/view/FCD108133CB2030B785366DBF529A892/S1351324918000128a.pdf/learning_quantification_from_images_a_structured_neural_architecture.pdf" target="_blank">[paper]</a> <a href="bib/jnle.bib" target="_blank">[bib]</a> [data] [code] <a href="https://arxiv.org/pdf/1704.02923.pdf" target="_blank">[arxiv]</a></li>

<!--
<li><b>Pezzelle, S.</b>, Marelli, M. (<i>under review</i>). Do Semantic Features Capture a Syntactic Classification of Compounds? Insights from Compositional Distributional Semantics.</li>
-->

			</ul>

                        <h2>2017</h2>
                        <ul>
                       

                        <li><b>Sorodoc, I.</b>,Lau, JH., Aletras, N., Baldwin, T. (2017). Multimodal topic labelling, <b>EACL2017</b>. <a href="https://www.aclweb.org/anthology/E17-2111" target="_blank">[paper]</a>  </li>                    
                        </ul>



                        <h2>2016</h2>
                        <ul>
                        <li>Pezzelle, S.,<b> Sorodoc, I.</b>, Herbelot, A., Bernardi, R. (2016). Imparare a quantificare guardando, <b>CLIC-it2016</b>. <a href="http://ceur-ws.org/Vol-1749/paper42.pdf" target="_blank">[paper]</a> <a href="/Publications/clic-it2016.pdf" target="_blank">[slides]</a></li>

                        <li><b>Sorodoc, I.</b>, Lazaridou, A., Boleda, G., Herbelot, A., Pezzelle, S., Bernardi, R. (2016). 'Look, some green circles!': Learning to quantify from images, <b>VL2016</b> at ACL2016. <a href="https://aclweb.org/anthology/W/W16/W16-3211.pdf" target="_blank">[paper]</a> <a href="circles_poster2016.pdf" target="_blank">[poster]</a> <a href="circles_slides2016.pdf" target="_blank">[slides]</a></li>
                        </ul>

                        <h2>2014</h2>
                        <ul>
                        <li><b>Sorodoc, I.</b> (2014) Aggregation methods for efficient collocation detection, LREC 2014 <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.674.1613&rep=rep1&type=pdf" target="_blank">[paper]</a></li>
                        </ul>

                       


                </div><!-- /.blurb -->
                </div><!-- /.container -->
                <footer>
                <ul>
        	<li><a href="mailto:ionut.sorodoc@gmail.com">ionut.sorodoc@gmail.com</a></li>
                        <li><a href="https://www.facebook.com/ionut.sorodoc" target="_blank">facebook</a></li>
			<li><a href="https://twitter.com/IonutSorodoc" target="_blank">twitter</a></li>
                        <li><a href="https://scholar.google.com/citations?user=XwT4ExYAAAAJ&hl=en" target="_blank">scholar</a></li>
                        </ul>
                </footer>
        </body>
</html>

